{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 2.2: The Quest For A Better Network\n",
    "\n",
    "In this assignment you will build a monster network to solve CIFAR10 image classification.\n",
    "\n",
    "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(please read it at least diagonally)\n",
    "\n",
    "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
    "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
    " \n",
    "## Grading\n",
    "* starting at zero points\n",
    "* +20% for describing your iteration path in a report below.\n",
    "* +20% for building a network that gets above 20% accuracy\n",
    "* +10% for beating each of these milestones on __TEST__ dataset:\n",
    "    * 50% (50% points)\n",
    "    * 60% (60% points)\n",
    "    * 65% (70% points)\n",
    "    * 70% (80% points)\n",
    "    * 75% (90% points)\n",
    "    * 80% (full points)\n",
    "    \n",
    "## Restrictions\n",
    "* Please do NOT use pre-trained networks for this assignment until you reach 80%.\n",
    " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the e-mail). After that, you can use whatever you want.\n",
    "* you __can__ use validation data for training, but you __can't'__ do anything with test data apart from running the evaluation procedure.\n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    " * __Network size__\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
    "\n",
    "   * Nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
    "\n",
    "\n",
    "### The main rule of prototyping: one change at a time\n",
    "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "   * You should certainly use adaptive optimizers\n",
    "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
    "     * Sometimes more batch normalization is better.\n",
    "   * __Regularize__ to prevent overfitting\n",
    "     * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "       * Can be done manually or like [this](https://discuss.pytorch.org/t/simple-l2-regularization/139/2).\n",
    "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
    "       * Don't overdo it. Check if it actually makes your network better\n",
    "   \n",
    "### Convolution architectures\n",
    "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
    "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
    "   * Please do try a few simple architectures before you go for resnet-152.\n",
    "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
    "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
    "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
    " \n",
    "   \n",
    "### Data augmemntation\n",
    "   * getting 5x as large dataset for free is a great \n",
    "     * Zoom-in+slice = move\n",
    "     * Rotate+zoom(to remove black stripes)\n",
    "     * Add Noize (gaussian or bernoulli)\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
    "   * A more advanced way is to use torchvision transforms:\n",
    "    ```\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root=path_to_cifar_like_in_seminar, train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    ```\n",
    "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
    "   \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "   \n",
    "There is a template for your solution below that you can opt to use or throw away and write it your way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((40000, 3, 32, 32), (40000,))\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"c66d071b-5ea9-4d3a-9b3d-af36dac50bb3\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"c66d071b-5ea9-4d3a-9b3d-af36dac50bb3\") === null) {\n",
       "                var notificationPayload = {\"body\": \"Cell execution has finished!\", \"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "from cifar import load_cifar10\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_cifar10(\"cifar_data\")\n",
    "class_names = np.array(['airplane','automobile ','bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck'])\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "   transforms.RandomCrop(32, padding=4),\n",
    "   transforms.RandomHorizontalFlip(),\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='cifar_data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_valid = transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "validset = torchvision.datasets.CIFAR10(root='cifar_data', train=True, download=True, transform=transform_valid)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fc1d52f84d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fc1e0920a50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(elem[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "( 0 , 0 ,.,.) = \n",
      " -2.4291 -2.4291 -2.4291  ...   0.6144  0.3042  0.2073\n",
      " -2.4291 -2.4291 -2.4291  ...   1.1571  0.9439  0.7501\n",
      " -2.4291 -2.4291 -2.4291  ...   1.4091  1.3704  1.3316\n",
      "           ...             ⋱             ...          \n",
      " -2.4291 -2.4291 -2.4291  ...  -0.6457 -0.6457 -0.6844\n",
      " -2.4291 -2.4291 -2.4291  ...  -2.4291 -2.4291 -2.4291\n",
      " -2.4291 -2.4291 -2.4291  ...  -2.4291 -2.4291 -2.4291\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      " -2.4183 -2.4183 -2.4183  ...   0.0794  0.0794  0.1384\n",
      " -2.4183 -2.4183 -2.4183  ...   0.0204  0.0008 -0.0386\n",
      " -2.4183 -2.4183 -2.4183  ...   0.0204 -0.0189 -0.0189\n",
      "           ...             ⋱             ...          \n",
      " -2.4183 -2.4183 -2.4183  ...  -0.3532 -0.3729 -0.3926\n",
      " -2.4183 -2.4183 -2.4183  ...  -2.4183 -2.4183 -2.4183\n",
      " -2.4183 -2.4183 -2.4183  ...  -2.4183 -2.4183 -2.4183\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      " -2.2214 -2.2214 -2.2214  ...   0.1394  0.1589  0.2369\n",
      " -2.2214 -2.2214 -2.2214  ...  -0.2509 -0.1533 -0.1143\n",
      " -2.2214 -2.2214 -2.2214  ...  -0.3484 -0.3094 -0.2704\n",
      "           ...             ⋱             ...          \n",
      " -2.2214 -2.2214 -2.2214  ...  -1.4410 -1.4605 -1.4410\n",
      " -2.2214 -2.2214 -2.2214  ...  -2.2214 -2.2214 -2.2214\n",
      " -2.2214 -2.2214 -2.2214  ...  -2.2214 -2.2214 -2.2214\n",
      "      ⋮  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      " -2.4291 -2.4291 -2.4291  ...  -2.4291 -2.4291 -2.4291\n",
      " -2.4291 -2.4291 -2.4291  ...  -2.4291 -2.4291 -2.4291\n",
      "  0.3817  1.3510  1.2347  ...  -2.4291 -2.4291 -2.4291\n",
      "           ...             ⋱             ...          \n",
      "  1.2153  1.2153  1.2347  ...  -2.4291 -2.4291 -2.4291\n",
      "  1.0602  1.1378  1.2735  ...  -2.4291 -2.4291 -2.4291\n",
      "  1.2735  1.2541  1.2735  ...  -2.4291 -2.4291 -2.4291\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      " -2.4183 -2.4183 -2.4183  ...  -2.4183 -2.4183 -2.4183\n",
      " -2.4183 -2.4183 -2.4183  ...  -2.4183 -2.4183 -2.4183\n",
      "  0.0991  0.8858  0.9841  ...  -2.4183 -2.4183 -2.4183\n",
      "           ...             ⋱             ...          \n",
      "  0.7678  0.7678  0.7874  ...  -2.4183 -2.4183 -2.4183\n",
      "  0.6104  0.6891  0.7874  ...  -2.4183 -2.4183 -2.4183\n",
      "  0.8268  0.8071  0.8071  ...  -2.4183 -2.4183 -2.4183\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      " -2.2214 -2.2214 -2.2214  ...  -2.2214 -2.2214 -2.2214\n",
      " -2.2214 -2.2214 -2.2214  ...  -2.2214 -2.2214 -2.2214\n",
      "  0.5296  1.3295  1.2905  ...  -2.2214 -2.2214 -2.2214\n",
      "           ...             ⋱             ...          \n",
      "  0.6466  0.6466  0.6466  ...  -2.2214 -2.2214 -2.2214\n",
      "  0.4905  0.5686  0.6076  ...  -2.2214 -2.2214 -2.2214\n",
      "  0.7052  0.6661  0.6466  ...  -2.2214 -2.2214 -2.2214\n",
      "      ⋮  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "  0.6725  0.4593  0.3430  ...  -0.9170 -0.9946 -2.4291\n",
      "  0.5562  0.4787  0.3624  ...  -1.3047 -1.3047 -2.4291\n",
      "  0.7113  0.6725  0.6144  ...  -1.1303 -1.1109 -2.4291\n",
      "           ...             ⋱             ...          \n",
      "  1.7193  1.7775  1.9713  ...   1.5448  1.5448 -2.4291\n",
      "  1.7193  1.7193  1.9325  ...   1.5642  1.6030 -2.4291\n",
      " -2.4291 -2.4291 -2.4291  ...  -2.4291 -2.4291 -2.4291\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  0.2958  0.0598 -0.0386  ...  -1.2186 -1.3169 -2.4183\n",
      "  0.1974  0.0991 -0.0189  ...  -1.6709 -1.6709 -2.4183\n",
      "  0.3351  0.2958  0.2368  ...  -1.5136 -1.5136 -2.4183\n",
      "           ...             ⋱             ...          \n",
      "  1.0038  1.1611  1.3578  ...   0.9448  0.9448 -2.4183\n",
      "  0.9841  1.1611  1.4561  ...   0.9251  0.9448 -2.4183\n",
      " -2.4183 -2.4183 -2.4183  ...  -2.4183 -2.4183 -2.4183\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      "  0.2759  0.0223 -0.0753  ...  -1.1873 -1.2849 -2.2214\n",
      "  0.1784  0.0613 -0.0558  ...  -1.6166 -1.6166 -2.2214\n",
      "  0.3149  0.2564  0.1979  ...  -1.4605 -1.4410 -2.2214\n",
      "           ...             ⋱             ...          \n",
      "  0.7637  0.9393  1.1539  ...   0.7247  0.7052 -2.2214\n",
      "  0.7052  0.8612  1.2124  ...   0.7247  0.7442 -2.2214\n",
      " -2.2214 -2.2214 -2.2214  ...  -2.2214 -2.2214 -2.2214\n",
      "...     \n",
      "      ⋮  \n",
      "\n",
      "(125, 0 ,.,.) = \n",
      " -0.6650 -0.7426 -0.9752  ...  -2.4291 -2.4291 -2.4291\n",
      " -0.8589 -1.1690 -1.1690  ...  -2.4291 -2.4291 -2.4291\n",
      " -0.6457 -0.7038 -0.8201  ...  -2.4291 -2.4291 -2.4291\n",
      "           ...             ⋱             ...          \n",
      "  2.5141  2.5141  2.5141  ...  -2.4291 -2.4291 -2.4291\n",
      " -2.4291 -2.4291 -2.4291  ...  -2.4291 -2.4291 -2.4291\n",
      " -2.4291 -2.4291 -2.4291  ...  -2.4291 -2.4291 -2.4291\n",
      "\n",
      "(125, 1 ,.,.) = \n",
      " -0.4122 -0.4909 -0.7269  ...  -2.4183 -2.4183 -2.4183\n",
      " -0.7466 -1.0219 -0.9826  ...  -2.4183 -2.4183 -2.4183\n",
      " -0.6876 -0.7072 -0.7662  ...  -2.4183 -2.4183 -2.4183\n",
      "           ...             ⋱             ...          \n",
      "  2.5378  2.5378  2.5378  ...  -2.4183 -2.4183 -2.4183\n",
      " -2.4183 -2.4183 -2.4183  ...  -2.4183 -2.4183 -2.4183\n",
      " -2.4183 -2.4183 -2.4183  ...  -2.4183 -2.4183 -2.4183\n",
      "\n",
      "(125, 2 ,.,.) = \n",
      " -0.1923 -0.2899 -0.5630  ...  -2.2214 -2.2214 -2.2214\n",
      " -0.4069 -0.6801 -0.6996  ...  -2.2214 -2.2214 -2.2214\n",
      " -0.4655 -0.4264 -0.4264  ...  -2.2214 -2.2214 -2.2214\n",
      "           ...             ⋱             ...          \n",
      "  2.6367  2.6172  2.6172  ...  -2.2214 -2.2214 -2.2214\n",
      " -2.2214 -2.2214 -2.2214  ...  -2.2214 -2.2214 -2.2214\n",
      " -2.2214 -2.2214 -2.2214  ...  -2.2214 -2.2214 -2.2214\n",
      "      ⋮  \n",
      "\n",
      "(126, 0 ,.,.) = \n",
      " -2.4291 -2.4291 -2.4291  ...  -2.4291 -2.4291 -2.4291\n",
      " -0.6457 -0.6844 -0.6457  ...  -2.4291 -2.4291 -2.4291\n",
      " -0.6650 -0.6069 -0.4712  ...  -2.4291 -2.4291 -2.4291\n",
      "           ...             ⋱             ...          \n",
      " -1.2272 -1.2466 -1.3047  ...  -2.4291 -2.4291 -2.4291\n",
      " -1.3047 -1.3047 -1.3047  ...  -2.4291 -2.4291 -2.4291\n",
      " -1.2854 -1.2854 -1.3047  ...  -2.4291 -2.4291 -2.4291\n",
      "\n",
      "(126, 1 ,.,.) = \n",
      " -2.4183 -2.4183 -2.4183  ...  -2.4183 -2.4183 -2.4183\n",
      " -0.4122 -0.4516 -0.4516  ...  -2.4183 -2.4183 -2.4183\n",
      " -0.4909 -0.5302 -0.4712  ...  -2.4183 -2.4183 -2.4183\n",
      "           ...             ⋱             ...          \n",
      " -1.6316 -1.6512 -1.7102  ...  -2.4183 -2.4183 -2.4183\n",
      " -1.7102 -1.7692 -1.7889  ...  -2.4183 -2.4183 -2.4183\n",
      " -1.6906 -1.7692 -1.7889  ...  -2.4183 -2.4183 -2.4183\n",
      "\n",
      "(126, 2 ,.,.) = \n",
      " -2.2214 -2.2214 -2.2214  ...  -2.2214 -2.2214 -2.2214\n",
      "  0.8807  0.8417  0.8807  ...  -2.2214 -2.2214 -2.2214\n",
      "  0.8807  0.8222  0.9003  ...  -2.2214 -2.2214 -2.2214\n",
      "           ...             ⋱             ...          \n",
      " -1.0118 -1.1483 -1.2849  ...  -2.2214 -2.2214 -2.2214\n",
      " -1.1873 -1.3434 -1.4410  ...  -2.2214 -2.2214 -2.2214\n",
      " -1.1483 -1.3239 -1.4605  ...  -2.2214 -2.2214 -2.2214\n",
      "      ⋮  \n",
      "\n",
      "(127, 0 ,.,.) = \n",
      " -2.4291 -2.4291 -1.5955  ...   1.5061  1.5448  1.5642\n",
      " -2.4291 -2.4291 -1.5374  ...   1.5836  1.6224  1.6418\n",
      " -2.4291 -2.4291 -1.6343  ...   1.5836  1.6030  1.6418\n",
      "           ...             ⋱             ...          \n",
      " -2.4291 -2.4291 -1.9638  ...   1.0796  1.1959  1.1765\n",
      " -2.4291 -2.4291 -2.0026  ...   1.1378  1.1765  1.0021\n",
      " -2.4291 -2.4291 -2.0801  ...   1.0796  0.9245  0.8664\n",
      "\n",
      "(127, 1 ,.,.) = \n",
      " -2.4183 -2.4183 -1.6119  ...   1.5938  1.6528  1.6724\n",
      " -2.4183 -2.4183 -1.4939  ...   1.6724  1.7314  1.7511\n",
      " -2.4183 -2.4183 -1.5529  ...   1.6528  1.7118  1.7314\n",
      "           ...             ⋱             ...          \n",
      " -2.4183 -2.4183 -2.0249  ...   1.2988  1.4364  1.3971\n",
      " -2.4183 -2.4183 -2.0446  ...   1.3381  1.4168  1.2398\n",
      " -2.4183 -2.4183 -2.1233  ...   1.3184  1.1808  1.1021\n",
      "\n",
      "(127, 2 ,.,.) = \n",
      " -2.2214 -2.2214 -1.2849  ...   1.5636  1.6026  1.6416\n",
      " -2.2214 -2.2214 -1.1483  ...   1.6221  1.6807  1.7197\n",
      " -2.2214 -2.2214 -1.2069  ...   1.6026  1.6612  1.7002\n",
      "           ...             ⋱             ...          \n",
      " -2.2214 -2.2214 -1.7727  ...   1.3880  1.5246  1.4856\n",
      " -2.2214 -2.2214 -1.8312  ...   1.4661  1.5246  1.3100\n",
      " -2.2214 -2.2214 -1.9287  ...   1.4075  1.2709  1.1734\n",
      "[torch.FloatTensor of size 128x3x32x32]\n",
      ", \n",
      " 9\n",
      " 7\n",
      " 6\n",
      " 9\n",
      " 1\n",
      " 5\n",
      " 7\n",
      " 5\n",
      " 0\n",
      " 2\n",
      " 4\n",
      " 7\n",
      " 0\n",
      " 0\n",
      " 4\n",
      " 8\n",
      " 1\n",
      " 7\n",
      " 6\n",
      " 2\n",
      " 4\n",
      " 1\n",
      " 9\n",
      " 1\n",
      " 3\n",
      " 2\n",
      " 9\n",
      " 4\n",
      " 2\n",
      " 7\n",
      " 2\n",
      " 9\n",
      " 0\n",
      " 2\n",
      " 2\n",
      " 9\n",
      " 6\n",
      " 3\n",
      " 0\n",
      " 7\n",
      " 5\n",
      " 8\n",
      " 1\n",
      " 2\n",
      " 6\n",
      " 7\n",
      " 5\n",
      " 0\n",
      " 6\n",
      " 1\n",
      " 6\n",
      " 3\n",
      " 3\n",
      " 1\n",
      " 5\n",
      " 3\n",
      " 6\n",
      " 7\n",
      " 4\n",
      " 4\n",
      " 0\n",
      " 1\n",
      " 7\n",
      " 7\n",
      " 3\n",
      " 9\n",
      " 5\n",
      " 9\n",
      " 1\n",
      " 0\n",
      " 3\n",
      " 0\n",
      " 5\n",
      " 8\n",
      " 0\n",
      " 3\n",
      " 4\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 7\n",
      " 1\n",
      " 8\n",
      " 5\n",
      " 1\n",
      " 2\n",
      " 2\n",
      " 3\n",
      " 9\n",
      " 7\n",
      " 0\n",
      " 1\n",
      " 6\n",
      " 1\n",
      " 8\n",
      " 5\n",
      " 3\n",
      " 6\n",
      " 9\n",
      " 2\n",
      " 8\n",
      " 7\n",
      " 4\n",
      " 7\n",
      " 1\n",
      " 2\n",
      " 1\n",
      " 3\n",
      " 7\n",
      " 9\n",
      " 2\n",
      " 5\n",
      " 0\n",
      " 5\n",
      " 9\n",
      " 5\n",
      " 1\n",
      " 5\n",
      " 8\n",
      " 5\n",
      " 9\n",
      " 2\n",
      " 5\n",
      " 5\n",
      " 4\n",
      " 7\n",
      " 3\n",
      " 3\n",
      "[torch.LongTensor of size 128]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for elem in trainloader:\n",
    "    print elem\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3,3)),\n",
    "    nn.Conv2d(in_channels=64, out_channels=16, kernel_size=(3,3)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.add_module('fst_conv', nn.Conv2d(in_channels=3, out_channels=20, kernel_size=(3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rrrr=model(Variable(torch.FloatTensor(X_train[0:3])))\n",
    "print(rrrr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module('fst_conv', nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3)))\n",
    "model.add_module('fst_mpool', nn.MaxPool2d(kernel_size=(2,2)))\n",
    "\n",
    "model.add_module('snd_conv', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2,2)))\n",
    "model.add_module('snd_mpool', nn.MaxPool2d(kernel_size=(2,2)))\n",
    "\n",
    "model.add_module('thrd_conv', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(2,2)))\n",
    "model.add_module('thrd_mpool', nn.MaxPool2d(kernel_size=(2,2)))\n",
    "\n",
    "model.add_module('flatten', Flatten())\n",
    "\n",
    "model.add_module('fst_ll', nn.Linear(in_features=1152, out_features=600))\n",
    "model.add_module('fst_nonlin', nn.Tanh())\n",
    "\n",
    "\n",
    "model.add_module('drop', nn.Dropout(p = 0.1))\n",
    "\n",
    "\n",
    "model.add_module('snd_ll', nn.Linear(in_features=600, out_features=10))\n",
    "model.add_module('out', nn.Softmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            Flatten(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256])\n"
     ]
    }
   ],
   "source": [
    "rrrr=model(Variable(torch.FloatTensor(X_train[0:3])))\n",
    "print(rrrr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add_module('liniya', nn.Linear(in_features=256, out_features=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add_module('soft', nn.Softmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "rrrr=model(Variable(torch.FloatTensor(X_train[0:3])))\n",
    "print(rrrr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, depth, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        # Model type specifies number of layers for CIFAR-10 model\n",
    "        assert (depth - 2) % 6 == 0, 'depth should be 6n+2'\n",
    "        n = (depth - 2) // 6\n",
    "\n",
    "        block = Bottleneck if depth >=44 else BasicBlock\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, n)\n",
    "        self.layer2 = self._make_layer(block, 32, n, stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, n, stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)    # 32x32\n",
    "\n",
    "        x = self.layer1(x)  # 32x32\n",
    "        x = self.layer2(x)  # 16x16\n",
    "        x = self.layer3(x)  # 8x8\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet model.\n",
    "    \"\"\"\n",
    "    return ResNet(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "setka = resnet(depth=20, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setka2 = resnet(depth=50, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setka2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setka.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = setka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = setka2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrrr=model2(Variable(torch.FloatTensor(X_train[0:3])).cuda())\n",
    "rrrr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = Variable(torch.FloatTensor(X_batch)).cuda()\n",
    "    y_batch = Variable(torch.LongTensor(y_batch)).cuda()\n",
    "    logits = model(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss2(X_batch, y_batch):\n",
    "    X_batch = Variable(torch.FloatTensor(X_batch)).cuda()\n",
    "    y_batch = Variable(torch.LongTensor(y_batch)).cuda()\n",
    "    logits = model2(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Training __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, y, batchsize):\n",
    "    indices = np.random.permutation(np.arange(len(X)))\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix], y[ix]\n",
    "        \n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss2 = []\n",
    "val_accuracy2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt2 = torch.optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 40.825s\n",
      "  training loss (in-iteration): \t0.909471\n",
      "  validation accuracy: \t\t\t68.30 %\n",
      "Epoch 2 of 100 took 40.984s\n",
      "  training loss (in-iteration): \t0.861573\n",
      "  validation accuracy: \t\t\t71.97 %\n",
      "Epoch 3 of 100 took 41.105s\n",
      "  training loss (in-iteration): \t0.800657\n",
      "  validation accuracy: \t\t\t72.51 %\n",
      "Epoch 4 of 100 took 41.333s\n",
      "  training loss (in-iteration): \t0.751132\n",
      "  validation accuracy: \t\t\t75.02 %\n",
      "Epoch 5 of 100 took 41.514s\n",
      "  training loss (in-iteration): \t0.702403\n",
      "  validation accuracy: \t\t\t76.34 %\n",
      "Epoch 6 of 100 took 41.425s\n",
      "  training loss (in-iteration): \t0.662269\n",
      "  validation accuracy: \t\t\t76.66 %\n",
      "Epoch 7 of 100 took 41.470s\n",
      "  training loss (in-iteration): \t0.617647\n",
      "  validation accuracy: \t\t\t77.33 %\n",
      "Epoch 8 of 100 took 41.465s\n",
      "  training loss (in-iteration): \t0.584012\n",
      "  validation accuracy: \t\t\t78.39 %\n",
      "Epoch 9 of 100 took 41.558s\n",
      "  training loss (in-iteration): \t0.557070\n",
      "  validation accuracy: \t\t\t80.83 %\n",
      "Epoch 10 of 100 took 41.512s\n",
      "  training loss (in-iteration): \t0.527755\n",
      "  validation accuracy: \t\t\t80.99 %\n",
      "Epoch 11 of 100 took 41.513s\n",
      "  training loss (in-iteration): \t0.504815\n",
      "  validation accuracy: \t\t\t81.25 %\n",
      "Epoch 12 of 100 took 41.484s\n",
      "  training loss (in-iteration): \t0.482900\n",
      "  validation accuracy: \t\t\t82.69 %\n",
      "Epoch 13 of 100 took 41.492s\n",
      "  training loss (in-iteration): \t0.464253\n",
      "  validation accuracy: \t\t\t83.60 %\n",
      "Epoch 14 of 100 took 41.451s\n",
      "  training loss (in-iteration): \t0.445020\n",
      "  validation accuracy: \t\t\t85.98 %\n",
      "Epoch 15 of 100 took 41.549s\n",
      "  training loss (in-iteration): \t0.420952\n",
      "  validation accuracy: \t\t\t85.88 %\n",
      "Epoch 16 of 100 took 41.493s\n",
      "  training loss (in-iteration): \t0.408621\n",
      "  validation accuracy: \t\t\t85.55 %\n",
      "Epoch 17 of 100 took 41.514s\n",
      "  training loss (in-iteration): \t0.386382\n",
      "  validation accuracy: \t\t\t85.98 %\n",
      "Epoch 18 of 100 took 41.548s\n",
      "  training loss (in-iteration): \t0.385775\n",
      "  validation accuracy: \t\t\t87.09 %\n",
      "Epoch 19 of 100 took 41.573s\n",
      "  training loss (in-iteration): \t0.362432\n",
      "  validation accuracy: \t\t\t88.63 %\n",
      "Epoch 20 of 100 took 41.513s\n",
      "  training loss (in-iteration): \t0.345164\n",
      "  validation accuracy: \t\t\t89.74 %\n",
      "Epoch 21 of 100 took 41.531s\n",
      "  training loss (in-iteration): \t0.336322\n",
      "  validation accuracy: \t\t\t88.45 %\n",
      "Epoch 22 of 100 took 41.509s\n",
      "  training loss (in-iteration): \t0.326229\n",
      "  validation accuracy: \t\t\t89.57 %\n",
      "Epoch 23 of 100 took 41.559s\n",
      "  training loss (in-iteration): \t0.312340\n",
      "  validation accuracy: \t\t\t89.39 %\n",
      "Epoch 24 of 100 took 41.510s\n",
      "  training loss (in-iteration): \t0.300247\n",
      "  validation accuracy: \t\t\t90.65 %\n",
      "Epoch 25 of 100 took 41.556s\n",
      "  training loss (in-iteration): \t0.293144\n",
      "  validation accuracy: \t\t\t89.31 %\n",
      "Epoch 26 of 100 took 41.459s\n",
      "  training loss (in-iteration): \t0.282556\n",
      "  validation accuracy: \t\t\t91.05 %\n",
      "Epoch 27 of 100 took 41.545s\n",
      "  training loss (in-iteration): \t0.277327\n",
      "  validation accuracy: \t\t\t91.32 %\n",
      "Epoch 28 of 100 took 41.541s\n",
      "  training loss (in-iteration): \t0.265339\n",
      "  validation accuracy: \t\t\t90.90 %\n",
      "Epoch 29 of 100 took 41.624s\n",
      "  training loss (in-iteration): \t0.255140\n",
      "  validation accuracy: \t\t\t89.66 %\n",
      "Epoch 30 of 100 took 41.524s\n",
      "  training loss (in-iteration): \t0.248425\n",
      "  validation accuracy: \t\t\t91.05 %\n",
      "Epoch 31 of 100 took 41.478s\n",
      "  training loss (in-iteration): \t0.243272\n",
      "  validation accuracy: \t\t\t91.87 %\n",
      "Epoch 32 of 100 took 41.506s\n",
      "  training loss (in-iteration): \t0.236563\n",
      "  validation accuracy: \t\t\t91.54 %\n",
      "Epoch 33 of 100 took 41.547s\n",
      "  training loss (in-iteration): \t0.225774\n",
      "  validation accuracy: \t\t\t92.46 %\n",
      "Epoch 34 of 100 took 41.585s\n",
      "  training loss (in-iteration): \t0.216738\n",
      "  validation accuracy: \t\t\t88.99 %\n",
      "Epoch 35 of 100 took 41.509s\n",
      "  training loss (in-iteration): \t0.206653\n",
      "  validation accuracy: \t\t\t91.95 %\n",
      "Epoch 36 of 100 took 41.548s\n",
      "  training loss (in-iteration): \t0.203382\n",
      "  validation accuracy: \t\t\t93.67 %\n",
      "Epoch 37 of 100 took 41.537s\n",
      "  training loss (in-iteration): \t0.202744\n",
      "  validation accuracy: \t\t\t93.24 %\n",
      "Epoch 38 of 100 took 41.566s\n",
      "  training loss (in-iteration): \t0.191670\n",
      "  validation accuracy: \t\t\t94.13 %\n",
      "Epoch 39 of 100 took 41.550s\n",
      "  training loss (in-iteration): \t0.187357\n",
      "  validation accuracy: \t\t\t93.54 %\n",
      "Epoch 40 of 100 took 41.586s\n",
      "  training loss (in-iteration): \t0.182379\n",
      "  validation accuracy: \t\t\t92.92 %\n",
      "Epoch 41 of 100 took 41.552s\n",
      "  training loss (in-iteration): \t0.176243\n",
      "  validation accuracy: \t\t\t93.01 %\n",
      "Epoch 42 of 100 took 41.558s\n",
      "  training loss (in-iteration): \t0.173623\n",
      "  validation accuracy: \t\t\t94.87 %\n",
      "Epoch 43 of 100 took 41.457s\n",
      "  training loss (in-iteration): \t0.171007\n",
      "  validation accuracy: \t\t\t93.66 %\n",
      "Epoch 44 of 100 took 41.574s\n",
      "  training loss (in-iteration): \t0.157665\n",
      "  validation accuracy: \t\t\t94.26 %\n",
      "Epoch 45 of 100 took 41.545s\n",
      "  training loss (in-iteration): \t0.157982\n",
      "  validation accuracy: \t\t\t94.11 %\n",
      "Epoch 46 of 100 took 41.511s\n",
      "  training loss (in-iteration): \t0.149958\n",
      "  validation accuracy: \t\t\t95.51 %\n",
      "Epoch 47 of 100 took 41.532s\n",
      "  training loss (in-iteration): \t0.154066\n",
      "  validation accuracy: \t\t\t93.45 %\n",
      "Epoch 48 of 100 took 41.373s\n",
      "  training loss (in-iteration): \t0.147785\n",
      "  validation accuracy: \t\t\t93.80 %\n",
      "Epoch 49 of 100 took 41.549s\n",
      "  training loss (in-iteration): \t0.138456\n",
      "  validation accuracy: \t\t\t95.10 %\n",
      "Epoch 50 of 100 took 41.428s\n",
      "  training loss (in-iteration): \t0.140039\n",
      "  validation accuracy: \t\t\t95.14 %\n",
      "Epoch 51 of 100 took 41.497s\n",
      "  training loss (in-iteration): \t0.136417\n",
      "  validation accuracy: \t\t\t94.80 %\n",
      "Epoch 52 of 100 took 41.408s\n",
      "  training loss (in-iteration): \t0.130128\n",
      "  validation accuracy: \t\t\t94.52 %\n",
      "Epoch 53 of 100 took 41.441s\n",
      "  training loss (in-iteration): \t0.132045\n",
      "  validation accuracy: \t\t\t96.17 %\n",
      "Epoch 54 of 100 took 41.521s\n",
      "  training loss (in-iteration): \t0.126173\n",
      "  validation accuracy: \t\t\t95.54 %\n",
      "Epoch 55 of 100 took 41.379s\n",
      "  training loss (in-iteration): \t0.120262\n",
      "  validation accuracy: \t\t\t96.06 %\n",
      "Epoch 56 of 100 took 41.437s\n",
      "  training loss (in-iteration): \t0.118967\n",
      "  validation accuracy: \t\t\t96.01 %\n",
      "Epoch 57 of 100 took 41.465s\n",
      "  training loss (in-iteration): \t0.117422\n",
      "  validation accuracy: \t\t\t97.22 %\n",
      "Epoch 58 of 100 took 41.516s\n",
      "  training loss (in-iteration): \t0.112055\n",
      "  validation accuracy: \t\t\t96.65 %\n",
      "Epoch 59 of 100 took 41.515s\n",
      "  training loss (in-iteration): \t0.112634\n",
      "  validation accuracy: \t\t\t95.99 %\n",
      "Epoch 60 of 100 took 41.666s\n",
      "  training loss (in-iteration): \t0.109894\n",
      "  validation accuracy: \t\t\t97.15 %\n",
      "Epoch 61 of 100 took 41.492s\n",
      "  training loss (in-iteration): \t0.111002\n",
      "  validation accuracy: \t\t\t96.85 %\n",
      "Epoch 62 of 100 took 41.467s\n",
      "  training loss (in-iteration): \t0.101037\n",
      "  validation accuracy: \t\t\t96.26 %\n",
      "Epoch 63 of 100 took 41.486s\n",
      "  training loss (in-iteration): \t0.102860\n",
      "  validation accuracy: \t\t\t96.76 %\n",
      "Epoch 64 of 100 took 41.475s\n",
      "  training loss (in-iteration): \t0.108111\n",
      "  validation accuracy: \t\t\t96.71 %\n",
      "Epoch 65 of 100 took 41.410s\n",
      "  training loss (in-iteration): \t0.095181\n",
      "  validation accuracy: \t\t\t96.33 %\n",
      "Epoch 66 of 100 took 41.475s\n",
      "  training loss (in-iteration): \t0.095984\n",
      "  validation accuracy: \t\t\t97.27 %\n",
      "Epoch 67 of 100 took 41.473s\n",
      "  training loss (in-iteration): \t0.098132\n",
      "  validation accuracy: \t\t\t97.62 %\n",
      "Epoch 68 of 100 took 41.375s\n",
      "  training loss (in-iteration): \t0.094759\n",
      "  validation accuracy: \t\t\t96.79 %\n",
      "Epoch 69 of 100 took 41.423s\n",
      "  training loss (in-iteration): \t0.088674\n",
      "  validation accuracy: \t\t\t97.18 %\n",
      "Epoch 70 of 100 took 41.428s\n",
      "  training loss (in-iteration): \t0.090724\n",
      "  validation accuracy: \t\t\t96.54 %\n",
      "Epoch 71 of 100 took 41.527s\n",
      "  training loss (in-iteration): \t0.090851\n",
      "  validation accuracy: \t\t\t97.35 %\n",
      "Epoch 72 of 100 took 41.574s\n",
      "  training loss (in-iteration): \t0.087056\n",
      "  validation accuracy: \t\t\t97.56 %\n",
      "Epoch 73 of 100 took 41.569s\n",
      "  training loss (in-iteration): \t0.084741\n",
      "  validation accuracy: \t\t\t97.23 %\n",
      "Epoch 74 of 100 took 41.558s\n",
      "  training loss (in-iteration): \t0.082157\n",
      "  validation accuracy: \t\t\t96.44 %\n",
      "Epoch 75 of 100 took 41.576s\n",
      "  training loss (in-iteration): \t0.083665\n",
      "  validation accuracy: \t\t\t95.99 %\n",
      "Epoch 76 of 100 took 41.528s\n",
      "  training loss (in-iteration): \t0.079832\n",
      "  validation accuracy: \t\t\t97.71 %\n",
      "Epoch 77 of 100 took 41.465s\n",
      "  training loss (in-iteration): \t0.077423\n",
      "  validation accuracy: \t\t\t98.22 %\n",
      "Epoch 78 of 100 took 41.555s\n",
      "  training loss (in-iteration): \t0.079210\n",
      "  validation accuracy: \t\t\t97.13 %\n",
      "Epoch 79 of 100 took 41.393s\n",
      "  training loss (in-iteration): \t0.078965\n",
      "  validation accuracy: \t\t\t97.85 %\n",
      "Epoch 97 of 100 took 41.521s\n",
      "  training loss (in-iteration): \t0.061197\n",
      "  validation accuracy: \t\t\t98.40 %\n",
      "Epoch 98 of 100 took 41.577s\n",
      "  training loss (in-iteration): \t0.061920\n",
      "  validation accuracy: \t\t\t98.12 %\n",
      "Epoch 99 of 100 took 41.535s\n",
      "  training loss (in-iteration): \t0.058293\n",
      "  validation accuracy: \t\t\t97.97 %\n",
      "Epoch 100 of 100 took 41.512s\n",
      "  training loss (in-iteration): \t0.053946\n",
      "  validation accuracy: \t\t\t98.24 %\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"ea896d8b-eb9e-4823-ad69-9130c4bd5429\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"ea896d8b-eb9e-4823-ad69-9130c4bd5429\") === null) {\n",
       "                var notificationPayload = {\"body\": \"Cell execution has finished!\", \"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "import time\n",
    "num_epochs = 100 # total amount of full passes over training data\n",
    "batch_size = 128  # number of samples processed in one SGD iteration\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    model2.train(True) # enable dropout / batch_norm training behavior\n",
    "    for X_batch, y_batch in trainloader:\n",
    "        # train on batch\n",
    "        loss = compute_loss2(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt2.step()\n",
    "        opt2.zero_grad()\n",
    "        train_loss2.append(loss.data.cpu().numpy()[0])\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    model2.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in validloader:\n",
    "        logits = model2(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "        val_accuracy2.append(np.mean(y_batch.numpy() == y_pred))\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss2[-len(X_train) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy2[-len(X_val) // batch_size :]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del validloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t89.79 %\n",
      "Achievement unlocked: 110lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "model2.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "for X_batch, y_batch in testloader:\n",
    "    logits = model2(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "    y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "    test_batch_acc.append(np.mean(y_batch.numpy() == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 95:\n",
    "    print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "elif test_accuracy * 100 > 90:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 80:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 70:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 60:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 8.118s\n",
      "  training loss (in-iteration): \t0.020525\n",
      "  validation accuracy: \t\t\t99.57 %\n",
      "Epoch 2 of 100 took 7.998s\n",
      "  training loss (in-iteration): \t0.016070\n",
      "  validation accuracy: \t\t\t99.58 %\n",
      "Epoch 3 of 100 took 7.930s\n",
      "  training loss (in-iteration): \t0.014265\n",
      "  validation accuracy: \t\t\t99.55 %\n",
      "Epoch 4 of 100 took 8.027s\n",
      "  training loss (in-iteration): \t0.012918\n",
      "  validation accuracy: \t\t\t99.67 %\n",
      "Epoch 5 of 100 took 8.099s\n",
      "  training loss (in-iteration): \t0.013578\n",
      "  validation accuracy: \t\t\t99.54 %\n",
      "Epoch 6 of 100 took 8.093s\n",
      "  training loss (in-iteration): \t0.014518\n",
      "  validation accuracy: \t\t\t99.62 %\n",
      "Epoch 7 of 100 took 7.974s\n",
      "  training loss (in-iteration): \t0.013638\n",
      "  validation accuracy: \t\t\t99.57 %\n",
      "Epoch 8 of 100 took 8.291s\n",
      "  training loss (in-iteration): \t0.011807\n",
      "  validation accuracy: \t\t\t99.52 %\n",
      "Epoch 9 of 100 took 8.060s\n",
      "  training loss (in-iteration): \t0.013541\n",
      "  validation accuracy: \t\t\t99.56 %\n",
      "Epoch 10 of 100 took 8.189s\n",
      "  training loss (in-iteration): \t0.012854\n",
      "  validation accuracy: \t\t\t99.56 %\n",
      "Epoch 11 of 100 took 8.036s\n",
      "  training loss (in-iteration): \t0.014154\n",
      "  validation accuracy: \t\t\t99.41 %\n",
      "Epoch 12 of 100 took 8.187s\n",
      "  training loss (in-iteration): \t0.014468\n",
      "  validation accuracy: \t\t\t99.50 %\n",
      "Epoch 13 of 100 took 8.218s\n",
      "  training loss (in-iteration): \t0.013496\n",
      "  validation accuracy: \t\t\t99.66 %\n",
      "Epoch 14 of 100 took 8.150s\n",
      "  training loss (in-iteration): \t0.016228\n",
      "  validation accuracy: \t\t\t99.38 %\n",
      "Epoch 15 of 100 took 8.225s\n",
      "  training loss (in-iteration): \t0.014934\n",
      "  validation accuracy: \t\t\t99.70 %\n",
      "Epoch 16 of 100 took 8.293s\n",
      "  training loss (in-iteration): \t0.016086\n",
      "  validation accuracy: \t\t\t99.36 %\n",
      "Epoch 17 of 100 took 8.153s\n",
      "  training loss (in-iteration): \t0.016505\n",
      "  validation accuracy: \t\t\t99.45 %\n",
      "Epoch 18 of 100 took 8.272s\n",
      "  training loss (in-iteration): \t0.018137\n",
      "  validation accuracy: \t\t\t99.27 %\n",
      "Epoch 19 of 100 took 8.234s\n",
      "  training loss (in-iteration): \t0.021571\n",
      "  validation accuracy: \t\t\t99.34 %\n",
      "Epoch 20 of 100 took 8.213s\n",
      "  training loss (in-iteration): \t0.022260\n",
      "  validation accuracy: \t\t\t99.32 %\n",
      "Epoch 21 of 100 took 8.326s\n",
      "  training loss (in-iteration): \t0.020291\n",
      "  validation accuracy: \t\t\t99.03 %\n",
      "Epoch 22 of 100 took 8.332s\n",
      "  training loss (in-iteration): \t0.018156\n",
      "  validation accuracy: \t\t\t99.30 %\n",
      "Epoch 23 of 100 took 8.231s\n",
      "  training loss (in-iteration): \t0.017347\n",
      "  validation accuracy: \t\t\t99.17 %\n",
      "Epoch 24 of 100 took 8.280s\n",
      "  training loss (in-iteration): \t0.021517\n",
      "  validation accuracy: \t\t\t99.44 %\n",
      "Epoch 25 of 100 took 8.247s\n",
      "  training loss (in-iteration): \t0.017423\n",
      "  validation accuracy: \t\t\t99.46 %\n",
      "Epoch 26 of 100 took 8.234s\n",
      "  training loss (in-iteration): \t0.020287\n",
      "  validation accuracy: \t\t\t99.26 %\n",
      "Epoch 27 of 100 took 8.319s\n",
      "  training loss (in-iteration): \t0.021726\n",
      "  validation accuracy: \t\t\t98.90 %\n",
      "Epoch 28 of 100 took 8.326s\n",
      "  training loss (in-iteration): \t0.022015\n",
      "  validation accuracy: \t\t\t99.22 %\n",
      "Epoch 29 of 100 took 8.330s\n",
      "  training loss (in-iteration): \t0.020251\n",
      "  validation accuracy: \t\t\t99.13 %\n",
      "Epoch 30 of 100 took 8.207s\n",
      "  training loss (in-iteration): \t0.023503\n",
      "  validation accuracy: \t\t\t99.17 %\n",
      "Epoch 31 of 100 took 8.231s\n",
      "  training loss (in-iteration): \t0.020842\n",
      "  validation accuracy: \t\t\t99.27 %\n",
      "Epoch 32 of 100 took 8.259s\n",
      "  training loss (in-iteration): \t0.020876\n",
      "  validation accuracy: \t\t\t98.77 %\n",
      "Epoch 33 of 100 took 8.313s\n",
      "  training loss (in-iteration): \t0.019295\n",
      "  validation accuracy: \t\t\t99.15 %\n",
      "Epoch 34 of 100 took 8.174s\n",
      "  training loss (in-iteration): \t0.019008\n",
      "  validation accuracy: \t\t\t99.34 %\n",
      "Epoch 35 of 100 took 8.207s\n",
      "  training loss (in-iteration): \t0.018233\n",
      "  validation accuracy: \t\t\t99.32 %\n",
      "Epoch 36 of 100 took 8.300s\n",
      "  training loss (in-iteration): \t0.020810\n",
      "  validation accuracy: \t\t\t99.32 %\n",
      "Epoch 37 of 100 took 8.437s\n",
      "  training loss (in-iteration): \t0.018274\n",
      "  validation accuracy: \t\t\t99.07 %\n",
      "Epoch 38 of 100 took 8.296s\n",
      "  training loss (in-iteration): \t0.019618\n",
      "  validation accuracy: \t\t\t99.48 %\n",
      "Epoch 39 of 100 took 8.293s\n",
      "  training loss (in-iteration): \t0.018307\n",
      "  validation accuracy: \t\t\t99.30 %\n",
      "Epoch 40 of 100 took 8.395s\n",
      "  training loss (in-iteration): \t0.019501\n",
      "  validation accuracy: \t\t\t99.09 %\n",
      "Epoch 41 of 100 took 8.207s\n",
      "  training loss (in-iteration): \t0.019686\n",
      "  validation accuracy: \t\t\t99.23 %\n",
      "Epoch 42 of 100 took 8.292s\n",
      "  training loss (in-iteration): \t0.018814\n",
      "  validation accuracy: \t\t\t99.35 %\n",
      "Epoch 43 of 100 took 8.160s\n",
      "  training loss (in-iteration): \t0.019251\n",
      "  validation accuracy: \t\t\t99.43 %\n",
      "Epoch 44 of 100 took 8.454s\n",
      "  training loss (in-iteration): \t0.018975\n",
      "  validation accuracy: \t\t\t99.39 %\n",
      "Epoch 45 of 100 took 8.286s\n",
      "  training loss (in-iteration): \t0.018219\n",
      "  validation accuracy: \t\t\t99.38 %\n",
      "Epoch 46 of 100 took 8.405s\n",
      "  training loss (in-iteration): \t0.021116\n",
      "  validation accuracy: \t\t\t99.18 %\n",
      "Epoch 47 of 100 took 8.183s\n",
      "  training loss (in-iteration): \t0.021666\n",
      "  validation accuracy: \t\t\t99.29 %\n",
      "Epoch 48 of 100 took 8.246s\n",
      "  training loss (in-iteration): \t0.021913\n",
      "  validation accuracy: \t\t\t99.03 %\n",
      "Epoch 49 of 100 took 8.084s\n",
      "  training loss (in-iteration): \t0.019778\n",
      "  validation accuracy: \t\t\t99.44 %\n",
      "Epoch 50 of 100 took 8.222s\n",
      "  training loss (in-iteration): \t0.015370\n",
      "  validation accuracy: \t\t\t99.30 %\n",
      "Epoch 51 of 100 took 8.203s\n",
      "  training loss (in-iteration): \t0.016776\n",
      "  validation accuracy: \t\t\t99.42 %\n",
      "Epoch 52 of 100 took 8.236s\n",
      "  training loss (in-iteration): \t0.018102\n",
      "  validation accuracy: \t\t\t99.20 %\n",
      "Epoch 53 of 100 took 8.409s\n",
      "  training loss (in-iteration): \t0.019215\n",
      "  validation accuracy: \t\t\t99.38 %\n",
      "Epoch 54 of 100 took 8.324s\n",
      "  training loss (in-iteration): \t0.018807\n",
      "  validation accuracy: \t\t\t99.50 %\n",
      "Epoch 55 of 100 took 8.205s\n",
      "  training loss (in-iteration): \t0.017005\n",
      "  validation accuracy: \t\t\t99.12 %\n",
      "Epoch 56 of 100 took 8.201s\n",
      "  training loss (in-iteration): \t0.016999\n",
      "  validation accuracy: \t\t\t99.44 %\n",
      "Epoch 57 of 100 took 8.296s\n",
      "  training loss (in-iteration): \t0.020587\n",
      "  validation accuracy: \t\t\t99.38 %\n",
      "Epoch 58 of 100 took 8.231s\n",
      "  training loss (in-iteration): \t0.020055\n",
      "  validation accuracy: \t\t\t99.14 %\n",
      "Epoch 59 of 100 took 8.194s\n",
      "  training loss (in-iteration): \t0.015301\n",
      "  validation accuracy: \t\t\t99.36 %\n",
      "Epoch 60 of 100 took 8.294s\n",
      "  training loss (in-iteration): \t0.017132\n",
      "  validation accuracy: \t\t\t99.47 %\n",
      "Epoch 61 of 100 took 8.203s\n",
      "  training loss (in-iteration): \t0.015925\n",
      "  validation accuracy: \t\t\t99.12 %\n",
      "Epoch 62 of 100 took 8.339s\n",
      "  training loss (in-iteration): \t0.016919\n",
      "  validation accuracy: \t\t\t99.46 %\n",
      "Epoch 63 of 100 took 8.510s\n",
      "  training loss (in-iteration): \t0.016741\n",
      "  validation accuracy: \t\t\t99.27 %\n",
      "Epoch 64 of 100 took 8.375s\n",
      "  training loss (in-iteration): \t0.020683\n",
      "  validation accuracy: \t\t\t99.01 %\n",
      "Epoch 65 of 100 took 8.260s\n",
      "  training loss (in-iteration): \t0.017719\n",
      "  validation accuracy: \t\t\t99.06 %\n",
      "Epoch 66 of 100 took 8.267s\n",
      "  training loss (in-iteration): \t0.019608\n",
      "  validation accuracy: \t\t\t99.13 %\n",
      "Epoch 67 of 100 took 8.338s\n",
      "  training loss (in-iteration): \t0.019349\n",
      "  validation accuracy: \t\t\t99.34 %\n",
      "Epoch 68 of 100 took 8.296s\n",
      "  training loss (in-iteration): \t0.018389\n",
      "  validation accuracy: \t\t\t99.40 %\n",
      "Epoch 69 of 100 took 8.495s\n",
      "  training loss (in-iteration): \t0.018926\n",
      "  validation accuracy: \t\t\t99.33 %\n",
      "Epoch 70 of 100 took 8.121s\n",
      "  training loss (in-iteration): \t0.019174\n",
      "  validation accuracy: \t\t\t99.23 %\n",
      "Epoch 71 of 100 took 8.273s\n",
      "  training loss (in-iteration): \t0.018985\n",
      "  validation accuracy: \t\t\t99.39 %\n",
      "Epoch 72 of 100 took 8.227s\n",
      "  training loss (in-iteration): \t0.016957\n",
      "  validation accuracy: \t\t\t99.39 %\n",
      "Epoch 73 of 100 took 8.512s\n",
      "  training loss (in-iteration): \t0.018029\n",
      "  validation accuracy: \t\t\t99.42 %\n",
      "Epoch 74 of 100 took 8.209s\n",
      "  training loss (in-iteration): \t0.018753\n",
      "  validation accuracy: \t\t\t99.39 %\n",
      "Epoch 75 of 100 took 8.188s\n",
      "  training loss (in-iteration): \t0.015986\n",
      "  validation accuracy: \t\t\t99.53 %\n",
      "Epoch 76 of 100 took 8.268s\n",
      "  training loss (in-iteration): \t0.015679\n",
      "  validation accuracy: \t\t\t99.17 %\n",
      "Epoch 77 of 100 took 8.329s\n",
      "  training loss (in-iteration): \t0.018932\n",
      "  validation accuracy: \t\t\t99.18 %\n",
      "Epoch 78 of 100 took 8.282s\n",
      "  training loss (in-iteration): \t0.019098\n",
      "  validation accuracy: \t\t\t99.01 %\n",
      "Epoch 79 of 100 took 8.155s\n",
      "  training loss (in-iteration): \t0.021505\n",
      "  validation accuracy: \t\t\t98.53 %\n",
      "Epoch 80 of 100 took 8.275s\n",
      "  training loss (in-iteration): \t0.017655\n",
      "  validation accuracy: \t\t\t99.19 %\n",
      "Epoch 81 of 100 took 8.256s\n",
      "  training loss (in-iteration): \t0.017335\n",
      "  validation accuracy: \t\t\t99.49 %\n",
      "Epoch 82 of 100 took 8.159s\n",
      "  training loss (in-iteration): \t0.017396\n",
      "  validation accuracy: \t\t\t99.46 %\n",
      "Epoch 83 of 100 took 8.249s\n",
      "  training loss (in-iteration): \t0.015842\n",
      "  validation accuracy: \t\t\t99.18 %\n",
      "Epoch 84 of 100 took 8.138s\n",
      "  training loss (in-iteration): \t0.014522\n",
      "  validation accuracy: \t\t\t99.48 %\n",
      "Epoch 85 of 100 took 8.403s\n",
      "  training loss (in-iteration): \t0.016568\n",
      "  validation accuracy: \t\t\t99.34 %\n",
      "Epoch 86 of 100 took 8.410s\n",
      "  training loss (in-iteration): \t0.016727\n",
      "  validation accuracy: \t\t\t99.53 %\n",
      "Epoch 87 of 100 took 8.471s\n",
      "  training loss (in-iteration): \t0.018527\n",
      "  validation accuracy: \t\t\t99.40 %\n",
      "Epoch 88 of 100 took 8.330s\n",
      "  training loss (in-iteration): \t0.019300\n",
      "  validation accuracy: \t\t\t99.17 %\n",
      "Epoch 89 of 100 took 8.301s\n",
      "  training loss (in-iteration): \t0.015588\n",
      "  validation accuracy: \t\t\t99.45 %\n",
      "Epoch 90 of 100 took 8.224s\n",
      "  training loss (in-iteration): \t0.017403\n",
      "  validation accuracy: \t\t\t99.40 %\n",
      "Epoch 91 of 100 took 8.139s\n",
      "  training loss (in-iteration): \t0.016982\n",
      "  validation accuracy: \t\t\t99.44 %\n",
      "Epoch 92 of 100 took 8.202s\n",
      "  training loss (in-iteration): \t0.018325\n",
      "  validation accuracy: \t\t\t99.30 %\n",
      "Epoch 93 of 100 took 8.206s\n",
      "  training loss (in-iteration): \t0.018730\n",
      "  validation accuracy: \t\t\t99.28 %\n",
      "Epoch 94 of 100 took 8.279s\n",
      "  training loss (in-iteration): \t0.015555\n",
      "  validation accuracy: \t\t\t99.57 %\n",
      "Epoch 95 of 100 took 8.237s\n",
      "  training loss (in-iteration): \t0.015393\n",
      "  validation accuracy: \t\t\t99.55 %\n",
      "Epoch 96 of 100 took 8.346s\n",
      "  training loss (in-iteration): \t0.016113\n",
      "  validation accuracy: \t\t\t99.59 %\n",
      "Epoch 97 of 100 took 8.232s\n",
      "  training loss (in-iteration): \t0.016231\n",
      "  validation accuracy: \t\t\t99.42 %\n",
      "Epoch 98 of 100 took 8.382s\n",
      "  training loss (in-iteration): \t0.016700\n",
      "  validation accuracy: \t\t\t99.35 %\n",
      "Epoch 99 of 100 took 8.284s\n",
      "  training loss (in-iteration): \t0.015842\n",
      "  validation accuracy: \t\t\t99.28 %\n",
      "Epoch 100 of 100 took 8.129s\n",
      "  training loss (in-iteration): \t0.014058\n",
      "  validation accuracy: \t\t\t99.37 %\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"fa03c622-9c1e-4385-9252-83faedcded62\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"fa03c622-9c1e-4385-9252-83faedcded62\") === null) {\n",
       "                var notificationPayload = {\"body\": \"Cell execution has finished!\", \"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "import time\n",
    "num_epochs = 10 # total amount of full passes over training data\n",
    "batch_size = 512  # number of samples processed in one SGD iteration\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for X_batch, y_batch in trainloader:\n",
    "        # train on batch\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.data.cpu().numpy()[0])\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in validloader:\n",
    "        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "        val_accuracy.append(np.mean(y_batch.numpy() == y_pred))\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy[-len(X_val) // batch_size :]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_test = transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='cifar_data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t88.86 %\n",
      "Achievement unlocked: 110lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "for X_batch, y_batch in testloader:\n",
    "    logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "    y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "    test_batch_acc.append(np.mean(y_batch.numpy() == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 95:\n",
    "    print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "elif test_accuracy * 100 > 90:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 80:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 70:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 60:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi, my name is `Semyon Fedotov`, and here's my story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В самом начале я решил попробовать простую сетку из свертки+пуллинга  + пара полносвязных слоев с релушками. В итоге обучения получил такой-себе результат (около 30% на тестовой выборке).\n",
    "----\n",
    "\n",
    "Далее, решил воспользоваться архитектурой, которую предлагали взять на семинарах(несколько (свертка+пуллинг) , нелинейности) смог добиться примерно 50%, что уже лучше, но недостаточно хорошо.\n",
    "\n",
    "----\n",
    "\n",
    "Поискал архитектуры помощней и решил использовать mini-resnet. Пытался на разных глубинах обучть, но результат примерно одинаковый(колебался вокруг 70%). Тут я решил воспользоваться аугментацией данных и качество сразу сильно выросло. Стало около 87%!!!. я попробовал пообучать больше эпох, разные батчи: 128, 256, 512. В итоге на тесте 88.86 при 300 эпохах\n",
    "\n",
    "-----\n",
    "\n",
    "Попытался сделать побольше слоев в финальной архитектуре, обучил, а при тестирование кончилось место на гпу(,  почистить что-то не вышло, пришлось размер бача на тесте сделать поменьше 500 -> 256. Качество 89.79 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
